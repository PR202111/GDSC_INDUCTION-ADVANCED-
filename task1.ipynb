{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1750664380325,
     "user": {
      "displayName": "Prashant Raj",
      "userId": "13131792136088247515"
     },
     "user_tz": -330
    },
    "id": "TgP0_fesZfQN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1750664380626,
     "user": {
      "displayName": "Prashant Raj",
      "userId": "13131792136088247515"
     },
     "user_tz": -330
    },
    "id": "yqL2a7cdZX-h"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentio(nn.Module):\n",
    "  def __init__(self, d_in,d_out,num_heads,dropout,context_len,qvk_bias=False):\n",
    "    super().__init__()\n",
    "    assert (d_out % num_heads == 0), \\\n",
    "      \"d_out must be divisible by num_heads\"\n",
    "\n",
    "    self.d_out = d_out\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_out // num_heads\n",
    "\n",
    "    self.W_query = nn.Linear(d_in,d_out,bias=qvk_bias)\n",
    "    self.W_value = nn.Linear(d_in,d_out,bias=qvk_bias)\n",
    "    self.W_key = nn.Linear(d_in,d_out,bias=qvk_bias)\n",
    "    self.out_proj = nn.Linear(d_out,d_out) #linear layer to combine head outputs\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer(\n",
    "        \"mask\", torch.triu(torch.ones(context_len,context_len),diagonal=1)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    b,num_tokens,d_in = x.shape\n",
    "\n",
    "    keys = self.W_key(x)\n",
    "    values= self.W_value(x)\n",
    "    queries = self.W_query(x)\n",
    "\n",
    "    keys = keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "    values = values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "\n",
    "    keys = keys.transpose(1,2)\n",
    "    values = values.transpose(1,2)\n",
    "    queries = queries.transpose(1,2)\n",
    "\n",
    "    attn_scores = queries @ keys.transpose(-2,-1)\n",
    "\n",
    "    mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "\n",
    "    attn_scores = attn_scores.masked_fill(mask_bool,-torch.inf)\n",
    "\n",
    "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim = -1)\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "    context_vec = (attn_weights @ values).transpose(1,2)\n",
    "\n",
    "    context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
    "    context_vec = self.out_proj(context_vec)\n",
    "\n",
    "    return context_vec\n",
    "    '''\n",
    "    view(...) → you're collecting opinions from 4 experts (attention heads) and writing them all down on one page.\n",
    "\n",
    "out_proj(...) → you're analyzing that combined report to make a decision, with learned weights.\n",
    "'''\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self,emb_dim):\n",
    "    super().__init__()\n",
    "    self.eps = 1e-5\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "  def forward(self,x):\n",
    "    mean = x.mean(dim = -1,keepdim = True)\n",
    "    var = x.var(dim = -1,keepdim = True,unbiased = False)\n",
    "\n",
    "    norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "    return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.register_buffer('c', torch.sqrt(torch.tensor(2.0 / torch.pi)))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return 0.5 * x * (1 + torch.tanh(self.c * (x + 0.044715 * x.pow(3))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.layers(x)\n",
    "\n",
    "\n",
    "\n",
    "#self, d_in,d_out,num_heads,dropout,context_len,qvk_bias=False\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "    self.att = MultiHeadAttentio(\n",
    "        d_in = cfg[\"emb_dim\"],\n",
    "        d_out = cfg[\"emb_dim\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout = cfg[\"drop_rate\"],\n",
    "        context_len = cfg[\"context_length\"],\n",
    "        qvk_bias = cfg[\"qvk_bias\"]\n",
    "    )\n",
    "    self.ff = FeedForward(cfg)\n",
    "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "  def forward(self, x):\n",
    "    shortcut = x\n",
    "    x = self.norm1(x)\n",
    "    x = self.att(x) # shape [batchsize, num_token, emb_size]\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = shortcut + x\n",
    "\n",
    "\n",
    "    shortcut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.ff(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    return x\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNjvJbxbr4E8gRktJmfQd0j",
   "gpuType": "T4",
   "mount_file_id": "1zcyzr88T5lVtF_neKKZDiTuYUeCQLWzf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
